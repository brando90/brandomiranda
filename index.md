---
layout: page
title: Brando Miranda
---

![me](/images/me_rains_suit.jpg)

- [My Google Scholar](https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en)
- [Stanford Profile](https://profiles.stanford.edu/brando-miranda?releaseVersion=9.9.0)
- [Twitter](https://twitter.com/BrandoHablando)
- [Linkedin](https://www.linkedin.com/in/brando-miranda-40821046/)
- [MIT and CBMM Profile](https://cbmm.mit.edu/about/people/miranda)
- [All social media - Professional and Personal](https://linktr.ee/ultimate_brando9)
- brando9 {at} stanford DoT Edu

<!-- - [(Maybe outdated) CV](/professional_documents/Brando_Miranda_long_CV.pdf) -->
<!-- Department of Computer Science
Gates Computer Science Building
353 Jane Stanford Way
Stanford, CA 94305 -->

-----

# Bio #
<!-- ref: https://chatgpt.com/c/68252ede-9708-8001-837e-5f556b905e53 -->

Brando Miranda is a Ph.D. student in Artificial Intelligence / Machine Learning (AI/ML) at [Stanford University](https://stanford.edu) under the supervision of [Professor Sanmi Koyejo](https://cs.stanford.edu/~sanmi/index.html) in the [Stanford Trustworthy AI Research (STAIR) group](https://cs.stanford.edu/~sanmi/index.html). 
Previously, he was a graduate researcher at the [University of Illinois Urbana-Champaign](https://illinois.edu/) and held research appointments at the Massachusetts Institute of Technology (MIT) — including a Research Assistantship in MIT’s Center for Brains, Minds and Machines (CBMM). 
He earned his Master of Engineering in Electrical Engineering and Computer Science at MIT, conducting deep-learning-theory research under [Professor Tomaso Poggio](https://mcgovern.mit.edu/profile/tomaso-poggio/) — who also supervised DeepMind co-founder Demis Hassabis during his post-doctoral fellowship.

Miranda’s research centers on data-centric machine learning for [Frontier Models (FMs)](https://openai.com/index/frontier-model-forum/), [Artificial General Intelligence (AGI)](https://en.wikipedia.org/wiki/Artificial_general_intelligence), and reasoning for mathematics and verified code. 
His work has earned the [NeurIPS Outstanding Main Track Paper Award](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/) (top 0.4%, 2 papers selected), the [International Journal of Automation & Computing “Most Cited Paper” Certificate](/professional_documents/Why_and_When_Can_Deep_but_Not_Shallow_networks_Avoid_the_Curse_of_Dimensionality_A_Review.jpg), two Honorable Mentions from the Ford Foundation Fellowship, the Saburo Muroga Computer Science Excellence Fellowship, the Stanford School of Engineering Fellowship, and selection as an EDGE Scholar at Stanford.

Miranda is more than a researcher—he is an innovator, communicator, and deeply passionate about the future of AI. 
As a Machine Learning Research Scientist Consultant at the venture-backed AI startup [Morph Labs](https://morph.so/blog/the-personal-ai-proof-engineer/), he made key contributions to the [Morph Prover](https://huggingface.co/morph-labs/morph-prover-v0-7b) model and helped launch [Moogle.ai](https://www.moogle.ai/), the first search engine for verified code in [Lean](https://leanprover-community.github.io/). 
He later applied the same expertise as an AI consultant to [Wise Agents](https://wiseagents.com/)—featured in [Forbes Mexico](https://www.forbes.com.mx/la-revolucion-digital-en-america-latina-desbloqueando-el-potencial-de-la-region/) — a Stanford spin-out leveraging AI agents to transform sales performance.


---

# Shorter Bio #

Brando Miranda is a Ph.D. student in Artificial Intelligence / Machine Learning at [Stanford University](https://stanford.edu), advised by [Prof. Sanmi Koyejo](https://cs.stanford.edu/~sanmi/index.html) in the Trustworthy AI Research (STAIR) group.
His work targets data-centric techniques for [Frontier Models](https://openai.com/index/frontier-model-forum/), [AGI](https://en.wikipedia.org/wiki/Artificial_general_intelligence), and machine-learning methods that generate or verify mathematical code.
Miranda received the [NeurIPS Outstanding Paper Award](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/) (top 0.4%) and other honors for his research.
In industry, he served as Machine-Learning Research Scientist Consultant at venture-backed [Morph Labs](https://morph.so/blog/the-personal-ai-proof-engineer/), helping build the [Morph Prover v0-7B](https://huggingface.co/morph-labs/morph-prover-v0-7b) and launch [Moogle.ai](https://www.moogle.ai/), the first search engine for verified code in [Lean](https://leanprover-community.github.io/).
He later brought the same expertise to [Wise Agents](https://wiseagents.com/), a Stanford spin-out that uses AI agents to transform sales performance.

<!-- [//]: # (https://cs.stanford.edu/~sanmi/preparation.html  working with me, TODO: )

[//]: # (Our Culture:
[//]: # (  
[//]: # (Our lab has a high technical bar, including a coding interview for all applicants. We constantly push each other to improve through continuous, regular feedback. We take mentorship seriously and are committed to students' success beyond their time in our lab. As a member of our lab, you will be working alongside prior USA Computing Olympiad (USACO) and International Mathematical Olympiad (IMO) participants. We have exceedingly high expectations of all collaborators and do not tolerate mediocrity. look for his lab page Sebastian Thrun's lab.)
[//]: # (someone from caltech or something like htat?) -->

<!-- ![me](/images/me_rains_suit.jpg){:class="img-responsive"} -->
<!-- ![me](/images/me_rains_suit.jpg) -->

---

# Selected Publications [ [Full List] ](https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en)

[//]: # (Note: * denotes equal contribution.)

Why Has Predicting Downstream Capabilities of Frontier AI Models with Scale Remained Elusive? (2025)
*Rylan Schaeffer, Hailey Schoelkopf, Brando Miranda, Gabriel Mukobi, Varun Madan, Adam Ibrahim, Herbie Bradley, Stella Biderman, Sanmi Koyejo.* 
[**[International Conference on Machine Learning (ICML) 2025]**](https://icml.cc/virtual/2025/poster/45753) <!-- TODO put icml paper link -->
[[**ICML Outstanding Paper Trustworthy Multi-modal Foundation Models and AI Agents (TiFA) Workshop 2024 award.**]](/professional_documents/tifa_award_elusive.png)
[**\[arxiv\]**](https://arxiv.org/pdf/2406.04391)

Putnam-AXIOM: A Functional & Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs (2024)
*Aryan Gulati, Brando Miranda, Eric Chen, Emily Xia, Kai Fronsdal, Bruno de Moraes Dumont, Sanmi Koyejo.* <!-- [**[arxiv]**](TODO) -->
[**[International Conference on Machine Learning (ICML) 2025]**](https://icml.cc/virtual/2025/poster/44232) <!-- TODO put icml paper link -->
[**[Neural Information Processing Systems (NeurIPS) Mathematics and AI (MATH-AI) Workshop 2024]**](https://neurips.cc/virtual/2024/98507) 
<!-- in case neurips link goes down for math-ai: https://openreview.net/forum?id=YXnwlZe0yf -->

Failures to Find Transferable Image Jailbreaks Between Vision-Language Models (2024)
*Rylan Schaeffer, Dan Valentine, Luke Bailey, James Chua, Cristobal Eyzaguirre, Zane Durante, Joe Benton, Brando Miranda, Henry Sleight, Tony Tong Wang, John Hughes, Rajashree Agrawal, Mrinank Sharma, Scott Emmons, Sanmi Koyejo, Ethan Perez.*
[**[International Conference on Machine Learning (ICML) 2025]**]()
[**[Neural Information Processing Systems (NeurIPS) Red Teaming GenAI Workshop Contributed Talk 2024]**](https://neurips.cc/virtual/2024/106794)
[**[International Conference on Learning Representations (ICLR) 2024]**](https://openreview.net/forum?id=wvFnqVVUhN)
<!-- https://slideslive.com/39031025/failures-to-find-transferable-image-jailbreaks-between-visionlanguage-models -->

Are Emergent Abilities of Large Language Models a Mirage? (2023)
*Rylan Schaeffer, Brando Miranda, Sanmi Koyejo.*
[**[Neural Information Processing Systems (NeurIPS) Outstanding Main Track Paper Award 2023 & NeurIPS Oral]**](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
[**[OpenReview]**](https://openreview.net/forum?id=ITw9edRDlD) 
[**[Stanford IEEE Invited Talk 2023]**](https://www.youtube.com/live/ypKwNrmuuPM?si=G8mfIdPaAFx82Jcl)
[**[arXiv]**](https://arxiv.org/abs/2304.15004)
<!-- [**[NeurIPS Oral]**](https://neurips.cc/virtual/2023/poster/72117) -->

<!-- Are Emergent Abilities of Large Language Models a Mirage?
*Rylan Schaeffer, Brando Miranda, Sanmi Koyejo.*
**Preprint & ICML Challenges in Deployable Generative AI Workshop 2023.**
[**[arXiv]**](https://arxiv.org/abs/2304.15004)  -->

Morph Prover v0 7b: The 1st Frontier Model for the Lean 4 Formal Verification Programming Language (2023)
[**[blog]**](https://morph.so/blog/the-personal-ai-proof-engineer/)
[**[Hugging Face Model Card]**](https://huggingface.co/morph-labs/morph-prover-v0-7b)

Is Pre-training Truly Better Than Meta-Learning? (2023)
*Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo.*
[**[International Conference on Machine Learning (ICML) Data-Centric Machine Learning Workshop 2023]**]
[**[Poster]**](https://docs.google.com/presentation/d/127Kmbi93dZOtGFnTEgyAvAWv4sX-RRPlEZh8p4zuUOw/edit?usp=sharing)
[**[arXiv]**](https://arxiv.org/abs/2306.13841)
[**[ICML PDF]**](https://dmlr.ai/assets/accepted-papers/117/CameraReady/MAML_vs_PT___NeurIPS__ICML_2023__Draft_2_.pdf)
<!-- [**[Code Coming Soon]**]() -->

Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data (2023)
*Alycia Lee\*, Brando Miranda\*, Patrick Yu, and Oluwasanmi Koyejo.*
[**[International Conference on Machine Learning (ICML) Data-Centric Machine Learning Workshop 2023 & ICML Challenges in Deployable Generative AI Workshop 2023]**]
[**[Poster]**](https://docs.google.com/presentation/d/1QF-S8URtOMWxsdaam_rVCWsotEC3CDsvQoNnboQ1CEI/edit?usp=sharing)
[**[arXiv]**](https://arxiv.org/abs/2306.13840)
[**[ICML PDF]**](https://dmlr.ai/assets/accepted-papers/113/CameraReady/ICML_2023_DMLR_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf)
[**[Code]**](https://github.com/alycialee/beyond-scale-language-data-diversity)

[//]: # ([**[Stanford Data Science Poster]**]&#40;https://docs.google.com/presentation/d/1W4biGEKO7jGOviClEtkqM6sscsth1mK9/edit?usp=sharing&ouid=111989168652781065814&rtpof=true&sd=true&#41;)
[//]: # ([**[Short 8 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf&#41;)
[//]: # ([**[Short 6 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__6pg_.pdf&#41;)
[//]: # (**Generative AI and Foundation Models Workshop 2023 - SAIL &#40;Stanford Artificial Intelligence Laboratory&#41;.**)
[//]: # ([**2023 Stanford Data Science Conference.**]&#40;https://datascience.stanford.edu/2023-stanford-data-science-conference&#41;)
[//]: # ([**[SAIL Poster]**]&#40;professional_documents/SAIL_2023_Poster.pdf&#41;)

The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence (2022)
*Brando Miranda, Patrick Yu, Yu-Xiong Wang, Oluwasanmi Koyejo.*
[**[Neural Information Processing Systems (NeurIPS) Meta-Learning Workshop 2022, Contributed Talk]**]
[**[OpenReview]**](https://openreview.net/forum?id=Z75fwzPdty)
[**[arXiv]**](https://arxiv.org/abs/2208.01545) 
[**[Poster]**](professional_documents/Poster_Low_Diversity____NeurIPS_WS_2022__Draft_2_.pdf)
[**[5 minute video]**](https://youtu.be/mM5vllz1hPg)
[**[15 minute video Contributed Talk]**](https://slideslive.com/38996684/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)
<!-- [**[Code Coming Soon]**]() -->

[//]: # ([**[PDF]**]&#40;https://openreview.net/forum?id=Z75fwzPdty&#41;)
[//]: # ([**[15 minute video Contributed Talk, pre-recording]**]&#40;https://youtu.be/3LfTWHIgmvM&#41;)
[//]: # ([**[Code, contact me for now, coming soon I hope!]**]&#40;&#41;)
[//]: # (5 min video from neurips)
[//]: # (https://slideslive.com/38994633/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)

Does MAML Only Work via Feature Re-use? A Data Centric Perspective (2021)
*Brando Miranda, Yu-Xiong Wang, Oluwasanmi Koyejo.*
**Preprint, Best research project award for graduate course CS 598 "Learning to Learn" by professor Y. Wang UIUC (December 2020).**
[**[arXiv]**](https://arxiv.org/abs/2112.13137)
[**[5 minute video]**](https://youtu.be/WyG6bwGnbGc)

[//]: # ([**[PDF]**]&#40;https://www.ideals.illinois.edu/handle/2142/109139&#41;)

Weight and Batch Normalization implement Classical Generalization Bounds (2019)
*Tomaso Poggio, Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Jack Hidary.*
[**[International Conference on Machine Learning (ICML) Workshop 2019]**]
[**[PDF]**](https://sites.google.com/view/icml2019-generalization/accepted-papers)

[//]: # ([**[PDF]**]&#40;/professional_documents/ICML2019_paper_53.pdf&#41;)

High-performance and scalable on-chip digital Fourier transform spectroscopy (2018)
*Derek M Kita, Brando Miranda, David Favela, David Bono, Jérôme Michon, Hongtao Lin, Tian Gu, Juejun Hu.*
**Nature Communications 2018.**
[**[PDF]**](https://www.nature.com/articles/s41467-018-06773-2)

Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review (2017)
*Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao.*
**International Journal of Automation and Computing 2017, Most Cited Paper Certificate awarded by International Journal of Automation & Computing (IJAC).**
[**[PDF]**](https://link.springer.com/article/10.1007/s11633-017-1054-2)
[**[Award]**](/professional_documents/Why_and_When_Can_Deep_but_Not_Shallow_networks_Avoid_the_Curse_of_Dimensionality_A_Review.jpg)

---

# Media Coverage

Below are selected links showcasing media coverage of some of my work:

[**Economic Report to the White House Washington (2024)**: Our work was cited in the 2024 Economic Report of the President. Direct quote: "The Report presents an overview of the nation's economic progress and makes the case for the Biden-Harris Administration's economic policy priorities."](https://www.whitehouse.gov/cea/written-materials/2024/03/21/the-2024-economic-report-of-the-president/) [**[Report]**](https://www.whitehouse.gov/wp-content/uploads/2024/03/ERP-2024-CHAPTER-7.pdf) [**[Screenshot]**](/images/white_house_labor_substitute_huam_rs_bm_sk.png)
<!-- [**[CopyReport]**](non_personal_documents/white house report schaeffer miranda koyejo cited.pdf) -->

[**The New York Times (June 2023)**: "Silicon Valley Confronts the Idea That the 'Singularity' Is Here"](https://www.nytimes.com/2023/06/11/technology/silicon-valley-confronts-the-idea-that-the-singularity-is-here.html)

[**Y Combinator News (May 2023)**: "Are emergent abilities of large language models a mirage?"](https://news.ycombinator.com/item?id=35768824)

[**Quanta Magazine (February 2024)**: "How Quickly Do Large Language Models Learn Unexpected Skills? - A new study suggests that so-called emergent abilities actually develop gradually and predictably, depending on how you measure them."](https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/)

[**Stanford's Institute for Human-Centered Artificial Intelligence (HAI) (May 2023)**: "AI's Ostensible Emergent Abilities Are a Mirage"](https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage)

[**Forbes (May 2023)**: "AI 'Emergent Abilities' Are A Mirage, Says AI Researcher"](https://www.forbes.com/sites/andreamorris/2023/05/09/ai-emergent-abilities-are-a-mirage-says-ai-researcher/?sh=1ec9b33f283f)

[**Andrew Ng (March 2024)**: Endorsed our paper & believes it is evidence that Artificial General Intelligence (AGI) won't come discontinuously, but instead, will come smoothly and predictably](https://x.com/AndrewYNg/status/1766554536192446957?s=20)

This work was also covered by: [Vice, Medium, Hacker News, NeurIPS blog, Reddit, and more](https://www.google.com/search?q=are+emergent+abilities+of+large+language+models+a+mirage), courtesy of Google search.

---

# Awards

- [ICML Outstanding Paper Trustworthy Multi-modal Foundation Models and AI Agents (TiFA) Workshop (July 2024)](/professional_documents/tifa_award_elusive.png)
- [Neural Information Processing Systems (NeurIPS) Outstanding Main Track Paper Award (December 2023) (top 0.4% and only 2 papers selected)](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
- EDGE Scholar, Stanford University (September 2022)
- Stanford School of Engineering Fellowship, Stanford University (September 2022)
- Best Research Project Award for Graduate Course CS 598 "Learning to Learn" by Professor Y. Wang, UIUC (December 2020)
- Hispanic Scholarship Fund (HSF) Scholar (2020)
- Honorable Mention, Ford Foundation Fellowship (2020, 2021)
- [Most Cited Paper Certificate awarded by International Journal of Automation & Computing (IJAC) (December 2019)](/professional_documents/Why_and_When_Can_Deep_but_Not_Shallow_networks_Avoid_the_Curse_of_Dimensionality_A_Review.jpg)
- Computer Science Excellence Saburo Muroga Endowed Fellow, UIUC (2019-2020)
- Sloan Scholar, Alfred P. Sloan Foundations Minority Ph.D. (MPHD) Program (2018-2019)
- Grainger Engineering SURGE Fellowship (2018-2019)
- Chopper Trading, LLC, Best Strategy Report Award, MIT Battle Code AI Competition (2013)
- MIT Mitchell B. Kaufman Memorial Scholarship (2012-2013, 2013-2014)
- MIT Eugene and Margaret (HM) McDermott Scholarship (2012-2013, 2013-2014)
- High Achievement Prize Award, Greengates School (2007, 2008, 2009, 2010), similar to Valedictorian
- Best All-Round Student Award, Greengates School (2010)
- Achievement Prize Award, Greengates School (2006)
- Certificate for Progress Award, Greengates School (2005)
