---
layout: page
title: Publications
---

[//]: # (For now check [my Google Scholar profile]&#40;https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en&#41; and [my home page when I was at MIT's Center for Brain Minds & Machines]&#40;https://cbmm.mit.edu/about/people/miranda&#41;.)


# Selected Publications [ [Full List] ](https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en)

[//]: # (Note: * denotes equal contribution.)

[//]: # (TODO put full list in my own website like Tony? perhaps not more work to manage)

Are Emergent Abilities of Large Language Models a Mirage?
*Rylan Schaeffer, Brando Miranda, Sanmi Koyejo.*
**Preprint & ICML Challenges in Deployable Generative AI Workshop 2023.**
[**[arXiv]**](https://arxiv.org/abs/2304.15004) 

Is Pre-training Truly Better Than Meta-Learning?
Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo.
**ICML Data-Centric Machine Learning Workshop 2023.**
[**[Poster]**](https://docs.google.com/presentation/d/127Kmbi93dZOtGFnTEgyAvAWv4sX-RRPlEZh8p4zuUOw/edit?usp=sharing)
[**[arxiv]**](https://arxiv.org/abs/2306.13841)
[**[ICML WS]**](https://dmlr.ai/assets/accepted-papers/117/CameraReady/MAML_vs_PT___NeurIPS__ICML_2023__Draft_2_.pdf)
[**[Code Coming Soon]**]()

Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.
*Alycia Lee\*, Brando Miranda\*, Patrick Yu, and Oluwasanmi Koyejo.*
**ICML Data-Centric Machine Learning Workshop 2023 & ICML Challenges in Deployable Generative AI Workshop 2023.**
[**[Poster]**](https://docs.google.com/presentation/d/1QF-S8URtOMWxsdaam_rVCWsotEC3CDsvQoNnboQ1CEI/edit?usp=sharing)
[**[arxiv]**](https://arxiv.org/abs/2306.13840)
[**[ICML WS]**](https://dmlr.ai/assets/accepted-papers/113/CameraReady/ICML_2023_DMLR_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf)
[**[Code]**](https://github.com/alycialee/beyond-scale-language-data-diversity)

[//]: # ([**[Stanford Data Science Poster]**]&#40;https://docs.google.com/presentation/d/1W4biGEKO7jGOviClEtkqM6sscsth1mK9/edit?usp=sharing&ouid=111989168652781065814&rtpof=true&sd=true&#41;)
[//]: # ([**[Short 8 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf&#41;)
[//]: # ([**[Short 6 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__6pg_.pdf&#41;)
[//]: # (**Generative AI and Foundation Models Workshop 2023 - SAIL &#40;Stanford Artificial Intelligence Laboratory&#41;.**)
[//]: # ([**2023 Stanford Data Science Conference.**]&#40;https://datascience.stanford.edu/2023-stanford-data-science-conference&#41;)
[//]: # ([**[SAIL Poster]**]&#40;professional_documents/SAIL_2023_Poster.pdf&#41;)

[The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence.](https://openreview.net/forum?id=Z75fwzPdty)
*Brando Miranda, Patrick Yu, Yu-Xiong Wang, Oluwasanmi Koyejo.*
**NeurIPS Meta-Learning Workshop 2022, Contributed Talk.**
[**[PDF]**](https://openreview.net/forum?id=Z75fwzPdty)
[**[arXiv]**](https://arxiv.org/abs/2208.01545) 
[**[Poster]**](professional_documents/Poster_Low_Diversity____NeurIPS_WS_2022__Draft_2_.pdf)
[**[5 minute video]**](https://youtu.be/mM5vllz1hPg)
[**[15 minute video Contributed Talk]**](https://slideslive.com/38996684/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)
[**[Code Coming Soon]**]()

[//]: # ([**[15 minute video Contributed Talk, pre-recording]**]&#40;https://youtu.be/3LfTWHIgmvM&#41;)
[//]: # ([**[Code, contact me for now, coming soon I hope!]**]&#40;&#41;)
[//]: # (5 min video from neurips)
[//]: # (https://slideslive.com/38994633/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)

[Does MAML Only Work via Feature Re-use? A Data Centric Perspective](https://arxiv.org/abs/2112.13137)
*Brando Miranda, Yu-Xiong Wang, Oluwasanmi Koyejo.*
**Preprint, Best research project award for graduate course CS 598 “Learning to Learn” by professor Y. Wang UIUC (December 2020).**
[**[arXiv]**](https://arxiv.org/abs/2112.13137)
[**[5 minute video]**](https://youtu.be/WyG6bwGnbGc)

[//]: # ([**[PDF]**]&#40;https://www.ideals.illinois.edu/handle/2142/109139&#41;)

[Weight and Batch Normalization implement Classical Generalization Bounds.](https://sites.google.com/view/icml2019-generalization/accepted-papers)
*Tomaso Poggio Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Jack Hidary.*
**ICML Workshop 2019**
[**[PDF]**](https://sites.google.com/view/icml2019-generalization/accepted-papers)

[//]: # ([**[PDF]**]&#40;/professional_documents/ICML2019_paper_53.pdf&#41;)

[High-performance and scalable on-chip digital Fourier transform spectroscopy.](https://www.nature.com/articles/s41467-018-06773-2)
*Derek M Kita, Brando Miranda, David Favela, David Bono, Jérôme Michon, Hongtao Lin, Tian Gu, Juejun Hu.*
**Nature Communications 2018.**
[**[PDF]**](https://www.nature.com/articles/s41467-018-06773-2)

[Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.](https://link.springer.com/article/10.1007/s11633-017-1054-2)
*Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao.*
**International Journal of Automation and Computing 2017, Most Cited Paper Certificate awarded by International Journal of Automation & Computing (IJAC).**
[**[PDF]**](https://link.springer.com/article/10.1007/s11633-017-1054-2)
[**[Award]**](/professional_documents/Why_and_When_Can_Deep_but_Not_Shallow_networks_Avoid_the_Curse_of_Dimensionality_A_Review.jpg)
