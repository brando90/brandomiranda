---
layout: page
title: Publications & Media Coverage
---

[//]: # (For now check [my Google Scholar profile]&#40;https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en&#41; and [my home page when I was at MIT's Center for Brain Minds & Machines]&#40;https://cbmm.mit.edu/about/people/miranda&#41;.)


# Selected Publications [ [Full List] ](https://scholar.google.com/citations?user=_NQJoBkAAAAJ&hl=en)

[//]: # (Note: * denotes equal contribution.)

[//]: # (TODO put full list in my own website like Tony? perhaps not more work to manage)

Are Emergent Abilities of Large Language Models a Mirage?
*Rylan Schaeffer, Brando Miranda, Sanmi Koyejo.*
[**[NeurIPS Outstanding Main Track Paper Award 2023 & NeurIPS Oral]**](https://blog.neurips.cc/2023/12/11/announcing-the-neurips-2023-paper-awards/)
[**[OpenReview]**](https://openreview.net/forum?id=ITw9edRDlD) 
[**[Stanford IEEE Invited Talk 2023]**](https://www.youtube.com/live/ypKwNrmuuPM?si=G8mfIdPaAFx82Jcl)
<!-- [**[NeurIPS Oral]**](https://neurips.cc/virtual/2023/poster/72117) -->

<!-- Are Emergent Abilities of Large Language Models a Mirage?
*Rylan Schaeffer, Brando Miranda, Sanmi Koyejo.*
**Preprint & ICML Challenges in Deployable Generative AI Workshop 2023.**
[**[arXiv]**](https://arxiv.org/abs/2304.15004)  -->

Is Pre-training Truly Better Than Meta-Learning?
Brando Miranda, Patrick Yu, Saumya Goyal, Yu-Xiong Wang, Sanmi Koyejo.
**ICML Data-Centric Machine Learning Workshop 2023.**
[**[Poster]**](https://docs.google.com/presentation/d/127Kmbi93dZOtGFnTEgyAvAWv4sX-RRPlEZh8p4zuUOw/edit?usp=sharing)
[**[arxiv]**](https://arxiv.org/abs/2306.13841)
[**[ICML PDF]**](https://dmlr.ai/assets/accepted-papers/117/CameraReady/MAML_vs_PT___NeurIPS__ICML_2023__Draft_2_.pdf)
[**[Code Coming Soon]**]()

Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data.
*Alycia Lee\*, Brando Miranda\*, Patrick Yu, and Oluwasanmi Koyejo.*
**ICML Data-Centric Machine Learning Workshop 2023 & ICML Challenges in Deployable Generative AI Workshop 2023.**
[**[Poster]**](https://docs.google.com/presentation/d/1QF-S8URtOMWxsdaam_rVCWsotEC3CDsvQoNnboQ1CEI/edit?usp=sharing)
[**[arxiv]**](https://arxiv.org/abs/2306.13840)
[**[ICML PDF]**](https://dmlr.ai/assets/accepted-papers/113/CameraReady/ICML_2023_DMLR_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf)
[**[Code]**](https://github.com/alycialee/beyond-scale-language-data-diversity)

[//]: # ([**[Stanford Data Science Poster]**]&#40;https://docs.google.com/presentation/d/1W4biGEKO7jGOviClEtkqM6sscsth1mK9/edit?usp=sharing&ouid=111989168652781065814&rtpof=true&sd=true&#41;)
[//]: # ([**[Short 8 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__8pg_.pdf&#41;)
[//]: # ([**[Short 6 page paper]**]&#40;professional_documents/ICML_2023_DeployGenAI_Workshop__Diversity_Coefficient___LLMs__6pg_.pdf&#41;)
[//]: # (**Generative AI and Foundation Models Workshop 2023 - SAIL &#40;Stanford Artificial Intelligence Laboratory&#41;.**)
[//]: # ([**2023 Stanford Data Science Conference.**]&#40;https://datascience.stanford.edu/2023-stanford-data-science-conference&#41;)
[//]: # ([**[SAIL Poster]**]&#40;professional_documents/SAIL_2023_Poster.pdf&#41;)

[The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence.](https://openreview.net/forum?id=Z75fwzPdty)
*Brando Miranda, Patrick Yu, Yu-Xiong Wang, Oluwasanmi Koyejo.*
**NeurIPS Meta-Learning Workshop 2022, Contributed Talk.**
[**[arXiv]**](https://arxiv.org/abs/2208.01545) 
[**[Poster]**](professional_documents/Poster_Low_Diversity____NeurIPS_WS_2022__Draft_2_.pdf)
[**[5 minute video]**](https://youtu.be/mM5vllz1hPg)
[**[15 minute video Contributed Talk]**](https://slideslive.com/38996684/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)
[**[Code Coming Soon]**]()

[//]: # ([**[PDF]**]&#40;https://openreview.net/forum?id=Z75fwzPdty&#41;)
[//]: # ([**[15 minute video Contributed Talk, pre-recording]**]&#40;https://youtu.be/3LfTWHIgmvM&#41;)
[//]: # ([**[Code, contact me for now, coming soon I hope!]**]&#40;&#41;)
[//]: # (5 min video from neurips)
[//]: # (https://slideslive.com/38994633/the-curse-of-low-task-diversity-on-the-failure-of-transfer-learning-to-outperform-maml-and-their-empirical-equivalence?ref=search-presentations-low+diversity)

[Does MAML Only Work via Feature Re-use? A Data Centric Perspective](https://arxiv.org/abs/2112.13137)
*Brando Miranda, Yu-Xiong Wang, Oluwasanmi Koyejo.*
**Preprint, Best research project award for graduate course CS 598 “Learning to Learn” by professor Y. Wang UIUC (December 2020).**
[**[arXiv]**](https://arxiv.org/abs/2112.13137)
[**[5 minute video]**](https://youtu.be/WyG6bwGnbGc)

[//]: # ([**[PDF]**]&#40;https://www.ideals.illinois.edu/handle/2142/109139&#41;)

[Weight and Batch Normalization implement Classical Generalization Bounds.](https://sites.google.com/view/icml2019-generalization/accepted-papers)
*Tomaso Poggio Andrzej Banburski, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Jack Hidary.*
**ICML Workshop 2019**
[**[PDF]**](https://sites.google.com/view/icml2019-generalization/accepted-papers)

[//]: # ([**[PDF]**]&#40;/professional_documents/ICML2019_paper_53.pdf&#41;)

[High-performance and scalable on-chip digital Fourier transform spectroscopy.](https://www.nature.com/articles/s41467-018-06773-2)
*Derek M Kita, Brando Miranda, David Favela, David Bono, Jérôme Michon, Hongtao Lin, Tian Gu, Juejun Hu.*
**Nature Communications 2018.**
[**[PDF]**](https://www.nature.com/articles/s41467-018-06773-2)

[Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review.](https://link.springer.com/article/10.1007/s11633-017-1054-2)
*Tomaso Poggio, Hrushikesh Mhaskar, Lorenzo Rosasco, Brando Miranda, Qianli Liao.*
**International Journal of Automation and Computing 2017, Most Cited Paper Certificate awarded by International Journal of Automation & Computing (IJAC).**
[**[PDF]**](https://link.springer.com/article/10.1007/s11633-017-1054-2)
[**[Award]**](/professional_documents/Why_and_When_Can_Deep_but_Not_Shallow_networks_Avoid_the_Curse_of_Dimensionality_A_Review.jpg)

# Media Coverage

Below are selected links showcasing media coverage of some of my work:

[**The New York Times**: Silicon Valley Confronts the Idea That the ‘Singularity’ Is Here](https://www.nytimes.com/2023/06/11/technology/silicon-valley-confronts-the-idea-that-the-singularity-is-here.html)

[**Y Combinator News**: Are emergent abilities of large language models a mirage?](https://news.ycombinator.com/item?id=35768824)

[**Quanta Magazine**: How Quickly Do Large Language Models Learn Unexpected Skills? - A new study suggests that so-called emergent abilities actually develop gradually and predictably, depending on how you measure them.](https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/)

[**Stanford's Institute for Human-Centered Artificial Intelligence (HAI)**: AI’s Ostensible Emergent Abilities Are a Mirage](https://hai.stanford.edu/news/ais-ostensible-emergent-abilities-are-mirage)

[**Forbes**: AI ‘Emergent Abilities’ Are A Mirage, Says AI Researcher](https://www.forbes.com/sites/andreamorris/2023/05/09/ai-emergent-abilities-are-a-mirage-says-ai-researcher/?sh=1ec9b33f283f)

This work was also covered by: [Vice, Medium, Hackwernews, NeurIPS blog, Reddit, and more](https://www.google.com/search?q=are+emergent+abilities+of+large+language+models+a+mirage&sca_esv=601452934&rlz=1C5CHFA_enUS741US741&sxsrf=ACQVn0-c2GdoTGcENwUnRQq9OL9o9oMnRw%3A1706215019783&ei=a8ayZdCpL_DnkPIP4pu2yA4&oq=are+emergent+abilities+of+large++a+mirage&gs_lp=Egxnd3Mtd2l6LXNlcnAiKWFyZSBlbWVyZ2VudCBhYmlsaXRpZXMgb2YgbGFyZ2UgIGEgbWlyYWdlKgIIADIGEAAYBxgeMgYQABgHGB4yBhAAGAcYHjIGEAAYBxgeSMc-UKcEWNw3cAB4AZABAJgBWqABqAmqAQIxN7gBA8gBAPgBAcICBBAAGEfCAgQQIxgnwgIKECMYgAQYigUYJ8ICFxAuGIAEGIoFGJECGLEDGIMBGMcBGNEDwgIKEAAYgAQYigUYQ8ICBRAAGIAEwgILEAAYgAQYsQMYgwHCAg4QLhiABBiKBRixAxiDAcICERAuGIAEGLEDGIMBGMcBGNEDwgIREC4YgAQYigUYkQIYsQMYgwHCAgsQLhiABBiKBRiRAsICCxAAGIAEGIoFGJECwgIOEAAYgAQYigUYsQMYgwHCAggQABiABBixA-IDBBgAIEGIBgGQBgg&sclient=gws-wiz-serp#ip=1), courtesy of Google search. 